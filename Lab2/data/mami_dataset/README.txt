# DATASET PARTITIONS

  Here you have two CSV files defining the official training and test sets
of MAMI, including all the necessary metadata. Among these metadata, it is
important to note that you already have the OCR-based transcription of the 
text content displayed in the meme, as well as the associated image caption
automatically generated by the pre-trained BLIP model. More specifically:

OCR model: https://cloud.google.com/vision/docs/ocr
BLIP model: https://huggingface.co/Salesforce/blip-image-captioning-large

## IMPORTANT

While for the laboratory sessions is not relevant, if you eventually used
the MAMI dataset, you must cite the original paper presenting the corpus.

This should be taken into account when participating in the EXIST2025 challenge,
so you will have to write a paper describing your appraoch.

## [OPTIONAL] ACCESS TO THE DATASET

You can use the memes images. It is optional, as indicated in the laboratory
bulletin. Please note the use of memes does not impact the evaluation of your
laboratory work.

Anyway, if you are keen to use the memes images, please place
here the MAMI dataset (./'TRAINING/' and './test/' directories).

To download the MAMI dataset and directly work with memes you can fill the following Google form:

https://forms.gle/AGWMiGicBHiQx4q98

, which has been copied from the official GitHub repository of the dataset:

https://github.com/MIND-Lab/SemEval2022-Task-5-Multimedia-Automatic-Misogyny-Identification-MAMI-

